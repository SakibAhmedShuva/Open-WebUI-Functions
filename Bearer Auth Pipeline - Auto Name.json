"""
title: My Hugging Face Space Integration
author: Assistant
date: 2025-07-27
version: 1.0.0
license: MIT
description: A pipe to connect Open WebUI with MyHuggingFace Hugging Face Space
requirements: requests
"""

import json
import requests
import time
from typing import List, Union, Generator, Dict
from pydantic import BaseModel, Field


class Pipe:
    class Valves(BaseModel):
        """Configuration for My Hugging Face Space API."""

        HUGGINGFACE_SPACE_URL: str = Field(
            default="https://dungpham519-rag.hf.space",
            description="Base URL for MyHuggingFace Hugging Face Space",
        )

        AUTHORIZATION_TOKEN: str = Field(
            default="LULA",
            description="Bearer token for API authentication",
        )

        TIMEOUT: int = Field(default=30, description="Request timeout in seconds")

        MAX_RETRIES: int = Field(
            default=3, description="Maximum number of retry attempts"
        )

        DEBUG: bool = Field(default=False, description="Enable debug logging")

        CACHE_MODELS: bool = Field(
            default=True, description="Cache model list to reduce API calls"
        )

        CACHE_DURATION: int = Field(
            default=300, description="Model cache duration in seconds (5 minutes)"
        )

    def __init__(self):
        """Initialize the My Hugging Face Space pipe."""
        self.type = "manifold"
        self.id = "MyHuggingFace-hf-space"
        self.name = "MyHuggingFace"

        # Rate limiting
        self.last_request_time = 0.0

        # Model caching
        self._cached_models = None
        self._cache_timestamp = 0

        # Initialize valves
        self.valves = self.Valves()

        # Set up headers
        self.headers = {
            "Authorization": f"Bearer {self.valves.AUTHORIZATION_TOKEN}",
            "Content-Type": "application/json",
            "Accept": "application/json",
        }

    def _debug(self, message: str, category: str = "INFO") -> None:
        """Debug logging helper."""
        if self.valves.DEBUG:
            print(f"[MyHuggingFacePipe][{category}] {message}")

    def _get_headers(self) -> Dict[str, str]:
        """Generate headers for API requests."""
        return {
            "Authorization": f"Bearer {self.valves.AUTHORIZATION_TOKEN}",
            "Content-Type": "application/json",
            "Accept": "application/json",
        }

    def _check_rate_limit(self):
        """Simple rate limiting to avoid overwhelming the API."""
        current_time = time.time()
        elapsed = current_time - self.last_request_time

        if elapsed < 1.0:  # Wait at least 1 second between requests
            time.sleep(1.0 - elapsed)

        self.last_request_time = time.time()

    def _is_cache_valid(self) -> bool:
        """Check if the model cache is still valid."""
        if not self.valves.CACHE_MODELS:
            return False

        current_time = time.time()
        return (
            self._cached_models is not None
            and current_time - self._cache_timestamp < self.valves.CACHE_DURATION
        )

    def _fetch_models_from_api(self) -> List[dict]:
        """Fetch available models from the API."""
        try:
            self._check_rate_limit()

            # Try the standard OpenAI-compatible models endpoint
            response = requests.get(
                f"{self.valves.HUGGINGFACE_SPACE_URL}/v1/models",
                headers=self._get_headers(),
                timeout=self.valves.TIMEOUT,
            )

            if response.status_code == 200:
                api_response = response.json()
                models = []

                # Handle OpenAI-style response
                if "data" in api_response:
                    for model_data in api_response["data"]:
                        model_id = model_data.get("id", "unknown")
                        model_name = model_data.get("id", model_id)

                        # Clean up model name for display
                        display_name = (
                            model_name.replace("/", "-").replace("_", " ").title()
                        )

                        models.append(
                            {
                                "id": f"MyHuggingFace-{model_id}",
                                "name": f"MyHuggingFace: {display_name}",
                                "description": f"Model: {model_name}",
                                "type": "chat",
                                "original_id": model_id,
                            }
                        )

                # Handle simple list response
                elif isinstance(api_response, list):
                    for model_name in api_response:
                        display_name = (
                            model_name.replace("/", "-").replace("_", " ").title()
                        )
                        models.append(
                            {
                                "id": f"MyHuggingFace-{model_name}",
                                "name": f"MyHuggingFace: {display_name}",
                                "description": f"Model: {model_name}",
                                "type": "chat",
                                "original_id": model_name,
                            }
                        )

                # Handle direct model object
                elif "models" in api_response:
                    for model_data in api_response["models"]:
                        model_id = model_data.get(
                            "id", model_data.get("name", "unknown")
                        )
                        display_name = (
                            model_id.replace("/", "-").replace("_", " ").title()
                        )

                        models.append(
                            {
                                "id": f"MyHuggingFace-{model_id}",
                                "name": f"MyHuggingFace: {display_name}",
                                "description": f"Model: {model_id}",
                                "type": "chat",
                                "original_id": model_id,
                            }
                        )

                if models:
                    self._debug(
                        f"Successfully fetched {len(models)} models from API", "MODELS"
                    )
                    return models
                else:
                    self._debug(
                        "No models found in API response, using fallback", "WARNING"
                    )

            else:
                self._debug(
                    f"Models API returned HTTP {response.status_code}", "WARNING"
                )

        except requests.exceptions.RequestException as e:
            self._debug(f"Failed to fetch models from API: {str(e)}", "ERROR")
        except Exception as e:
            self._debug(f"Unexpected error fetching models: {str(e)}", "ERROR")

        # Return fallback models if API call fails
        return self._get_fallback_models()

    def _get_fallback_models(self) -> List[dict]:
        """Return fallback models if API is unavailable."""
        self._debug("Using fallback models", "FALLBACK")
        return [
            {
                "id": "MyHuggingFace-default",
                "name": "MyHuggingFace: Default Model",
                "description": "Default model from MyHuggingFace Hugging Face Space",
                "type": "chat",
                "original_id": "default",
            },
            {
                "id": "MyHuggingFace-chat",
                "name": "MyHuggingFace: Chat Model",
                "description": "Chat model from MyHuggingFace Space",
                "type": "chat",
                "original_id": "chat",
            },
        ]

    def pipes(self) -> List[dict]:
        """
        Return available models/pipes from My Hugging Face Space.
        This method defines what models appear in the Open WebUI interface.
        """
        # Check if we have valid cached models
        if self._is_cache_valid():
            self._debug(
                f"Using cached models ({len(self._cached_models)} models)", "CACHE"
            )
            return self._cached_models

        # Fetch models from API
        models = self._fetch_models_from_api()

        # Cache the results
        if self.valves.CACHE_MODELS:
            self._cached_models = models
            self._cache_timestamp = time.time()
            self._debug(f"Cached {len(models)} models", "CACHE")

        return models

    def _get_original_model_id(self, openwebui_model_id: str) -> str:
        """Extract original model ID from OpenWebUI model ID."""
        if openwebui_model_id.startswith("MyHuggingFace-"):
            return openwebui_model_id[14:]  # Remove "MyHuggingFace-" prefix
        return openwebui_model_id

    def pipe(self, body: dict) -> Union[str, Generator[str, None, None]]:
        """
        Main request handler for chat completions.

        Args:
            body: Request body containing model and messages

        Returns:
            Response content either as string or streaming generator
        """
        try:
            model = body.get("model", "MyHuggingFace-default")
            messages = body.get("messages", [])
            stream = body.get("stream", False)

            # Convert OpenWebUI model ID back to original model ID
            original_model_id = self._get_original_model_id(model)

            self._debug(
                f"Processing request for model: {model} -> {original_model_id}",
                "REQUEST",
            )

            # Prepare payload for the MyHuggingFace API
            payload = {
                "model": original_model_id,
                "messages": messages,
                "stream": stream,
                "temperature": body.get("temperature", 0.7),
                "max_tokens": body.get("max_tokens", 1000),
                "top_p": body.get("top_p", 1.0),
            }

            if stream:
                return self._handle_streaming(payload)
            else:
                return self._handle_non_streaming(payload)

        except Exception as e:
            self._debug(f"Request failed: {str(e)}", "ERROR")
            return f"Error: {str(e)}"

    def _handle_streaming(self, payload: dict) -> Generator[str, None, None]:
        """Handle streaming chat completion request."""
        self._check_rate_limit()

        try:
            response = requests.post(
                f"{self.valves.HUGGINGFACE_SPACE_URL}/v1/chat/completions",
                json=payload,
                headers=self._get_headers(),
                stream=True,
                timeout=self.valves.TIMEOUT,
            )

            if response.status_code != 200:
                yield f"Error: HTTP {response.status_code}: {response.text}"
                return

            # Process streaming response
            for line in response.iter_lines():
                if line:
                    line = line.decode("utf-8").strip()

                    if line.startswith("data: "):
                        data = line[6:]  # Remove 'data: ' prefix

                        if data.strip() == "[DONE]":
                            break

                        try:
                            json_data = json.loads(data)

                            if "choices" in json_data and len(json_data["choices"]) > 0:
                                delta = json_data["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    yield content

                        except json.JSONDecodeError:
                            if self.valves.DEBUG:
                                self._debug(f"Failed to parse JSON: {data}", "ERROR")
                            continue

        except requests.exceptions.RequestException as e:
            self._debug(f"Streaming error: {str(e)}", "ERROR")
            yield f"Error: {str(e)}"

    def _handle_non_streaming(self, payload: dict) -> str:
        """Handle non-streaming chat completion request."""
        self._check_rate_limit()

        for attempt in range(self.valves.MAX_RETRIES):
            try:
                response = requests.post(
                    f"{self.valves.HUGGINGFACE_SPACE_URL}/v1/chat/completions",
                    json=payload,
                    headers=self._get_headers(),
                    timeout=self.valves.TIMEOUT,
                )

                if response.status_code == 200:
                    result = response.json()

                    # Handle OpenAI-style response
                    if "choices" in result and len(result["choices"]) > 0:
                        return result["choices"][0]["message"]["content"]
                    # Handle simple response
                    elif "response" in result:
                        return result["response"]
                    # Handle raw response
                    else:
                        return str(result)
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text}"
                    if attempt == self.valves.MAX_RETRIES - 1:
                        return f"Error: {error_msg}"
                    self._debug(f"Attempt {attempt + 1} failed: {error_msg}", "ERROR")

            except requests.exceptions.Timeout:
                if attempt == self.valves.MAX_RETRIES - 1:
                    return "Error: Request timed out"
                self._debug(f"Attempt {attempt + 1} timed out", "ERROR")

            except requests.exceptions.ConnectionError:
                if attempt == self.valves.MAX_RETRIES - 1:
                    return "Error: Could not connect to MyHuggingFace service"
                self._debug(f"Attempt {attempt + 1} connection failed", "ERROR")

            except Exception as e:
                if attempt == self.valves.MAX_RETRIES - 1:
                    return f"Error: {str(e)}"
                self._debug(f"Attempt {attempt + 1} failed: {str(e)}", "ERROR")

        return "Error: All retry attempts failed"

    def test_connection(self) -> dict:
        """Test connection to the My Hugging Face Space."""
        try:
            response = requests.get(
                f"{self.valves.HUGGINGFACE_SPACE_URL}/health",
                headers=self._get_headers(),
                timeout=10,
            )

            if response.status_code == 200:
                return {"status": "success", "message": "Connection successful"}
            else:
                return {
                    "status": "error",
                    "message": f"Health check failed: HTTP {response.status_code}",
                }
        except Exception as e:
            return {"status": "error", "message": f"Connection failed: {str(e)}"}

    def refresh_models(self) -> dict:
        """Manually refresh the model cache."""
        try:
            self._cached_models = None
            self._cache_timestamp = 0
            models = self.pipes()
            return {
                "status": "success",
                "message": f"Successfully refreshed {len(models)} models",
            }
        except Exception as e:
            return {"status": "error", "message": f"Failed to refresh models: {str(e)}"}
