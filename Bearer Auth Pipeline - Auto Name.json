"""
title: My Hugging Face Space Integration
author: Assistant
date: 2025-07-27
version: 1.0.0
license: MIT
description: A pipe to connect Open WebUI with MyHuggingFace Hugging Face Space
requirements: requests
"""

import json
import requests
import time
from typing import List, Union, Generator, Dict
from pydantic import BaseModel, Field

# ========================================
# CONFIGURATION VARIABLES - EDIT HERE
# ========================================

# Bot/Service Identity (BOT_NAME will be auto-generated from URL)
BOT_NAME = None  # Will be auto-detected from HUGGINGFACE_SPACE_URL
BOT_DISPLAY_NAME = "MyBot: "  # Fixed display name to avoid duplication
BOT_ID_SUFFIX = "hf-space"  # Will be combined with detected name
BOT_TYPE = "manifold"

# API Configuration
DEFAULT_SPACE_URL = "https://dungpham519-rag.hf.space"
DEFAULT_AUTH_TOKEN = "LULA"
DEFAULT_TIMEOUT = 30
DEFAULT_MAX_RETRIES = 3

# Cache Settings
DEFAULT_CACHE_MODELS = True
DEFAULT_CACHE_DURATION = 300  # 5 minutes in seconds

# Debug and Rate Limiting
DEFAULT_DEBUG = False
RATE_LIMIT_DELAY = 1.0  # seconds between requests

# API Endpoints
MODELS_ENDPOINT = "/v1/models"
CHAT_ENDPOINT = "/v1/chat/completions"
HEALTH_ENDPOINT = "/health"

# Model Naming (will be auto-generated)
MODEL_PREFIX = None  # Will be set based on detected BOT_NAME
MODEL_DISPLAY_PREFIX = ""  # Empty to avoid duplication since BOT_NAME is used elsewhere

# Default Model Configuration
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 1000
DEFAULT_TOP_P = 1.0

# Fallback Models (will be populated after BOT_NAME detection)
FALLBACK_MODELS = []  # Will be set in __init__ after name detection

# ========================================
# MAIN PIPE CLASS
# ========================================


class Pipe:
    class Valves(BaseModel):
        """Configuration for My Hugging Face Space API."""

        HUGGINGFACE_SPACE_URL: str = Field(
            default=DEFAULT_SPACE_URL,
            description="Base URL for MyHuggingFace Hugging Face Space",
        )

        AUTHORIZATION_TOKEN: str = Field(
            default=DEFAULT_AUTH_TOKEN,
            description="Bearer token for API authentication",
        )

        TIMEOUT: int = Field(
            default=DEFAULT_TIMEOUT, description="Request timeout in seconds"
        )

        MAX_RETRIES: int = Field(
            default=DEFAULT_MAX_RETRIES, description="Maximum number of retry attempts"
        )

        DEBUG: bool = Field(default=DEFAULT_DEBUG, description="Enable debug logging")

        CACHE_MODELS: bool = Field(
            default=DEFAULT_CACHE_MODELS,
            description="Cache model list to reduce API calls",
        )

        CACHE_DURATION: int = Field(
            default=DEFAULT_CACHE_DURATION,
            description="Model cache duration in seconds (5 minutes)",
        )

    def __init__(self):
        """Initialize the My Hugging Face Space pipe."""
        # Initialize valves first to get the URL
        self.valves = self.Valves()

        # Auto-detect bot name from URL
        self._detect_bot_name()

        # Set up identity
        self.type = BOT_TYPE
        self.id = f"{self.bot_name}-{BOT_ID_SUFFIX}"
        self.name = BOT_DISPLAY_NAME  # Use fixed display name

        # Set up model naming
        self.model_prefix = f"{self.bot_name}-"

        # Set up fallback models
        self._setup_fallback_models()

        # Rate limiting
        self.last_request_time = 0.0

        # Model caching
        self._cached_models = None
        self._cache_timestamp = 0

        # Set up headers
        self.headers = {
            "Authorization": f"Bearer {self.valves.AUTHORIZATION_TOKEN}",
            "Content-Type": "application/json",
            "Accept": "application/json",
        }

    def _detect_bot_name(self):
        """Auto-detect bot name from the Hugging Face Space URL."""
        url = self.valves.HUGGINGFACE_SPACE_URL

        # Extract name from URL like https://dungpham519-rag.hf.space
        try:
            # Remove protocol
            if "://" in url:
                url = url.split("://")[1]

            # Get the subdomain part before .hf.space
            if ".hf.space" in url:
                subdomain = url.split(".hf.space")[0]
                # Clean up the name and capitalize appropriately
                bot_name = subdomain.replace("-", " ").title().replace(" ", "")
                self.bot_name = bot_name
                self._debug(f"Auto-detected bot name: {self.bot_name}", "INIT")
            else:
                # Fallback: try to extract from domain
                domain_parts = url.split(".")
                if len(domain_parts) > 0:
                    self.bot_name = domain_parts[0].replace("-", "").title()
                else:
                    self.bot_name = "HuggingFaceSpace"
                self._debug(f"Using fallback bot name: {self.bot_name}", "INIT")

        except Exception as e:
            self.bot_name = "HuggingFaceSpace"
            self._debug(
                f"Failed to detect bot name, using default: {self.bot_name}. Error: {str(e)}",
                "WARNING",
            )

    def _setup_fallback_models(self):
        """Set up fallback models using the detected bot name."""
        self.fallback_models = [
            {
                "id": f"{self.model_prefix}default",
                "name": "Default Model",  # FIXED: Removed duplicate prefix
                "description": f"Default model from {self.bot_name} Hugging Face Space",
                "type": "chat",
                "original_id": "default",
            },
            {
                "id": f"{self.model_prefix}chat",
                "name": "Chat Model",  # FIXED: Removed duplicate prefix
                "description": f"Chat model from {self.bot_name} Space",
                "type": "chat",
                "original_id": "chat",
            },
        ]

    def _debug(self, message: str, category: str = "INFO") -> None:
        """Debug logging helper."""
        if self.valves.DEBUG:
            bot_name = getattr(self, "bot_name", "HuggingFaceSpace")
            print(f"[{bot_name}Pipe][{category}] {message}")

    def _get_headers(self) -> Dict[str, str]:
        """Generate headers for API requests."""
        return {
            "Authorization": f"Bearer {self.valves.AUTHORIZATION_TOKEN}",
            "Content-Type": "application/json",
            "Accept": "application/json",
        }

    def _check_rate_limit(self):
        """Simple rate limiting to avoid overwhelming the API."""
        current_time = time.time()
        elapsed = current_time - self.last_request_time

        if elapsed < RATE_LIMIT_DELAY:
            time.sleep(RATE_LIMIT_DELAY - elapsed)

        self.last_request_time = time.time()

    def _is_cache_valid(self) -> bool:
        """Check if the model cache is still valid."""
        if not self.valves.CACHE_MODELS:
            return False

        current_time = time.time()
        return (
            self._cached_models is not None
            and current_time - self._cache_timestamp < self.valves.CACHE_DURATION
        )

    def _fetch_models_from_api(self) -> List[dict]:
        """Fetch available models from the API."""
        try:
            self._check_rate_limit()

            # Try the standard OpenAI-compatible models endpoint
            response = requests.get(
                f"{self.valves.HUGGINGFACE_SPACE_URL}{MODELS_ENDPOINT}",
                headers=self._get_headers(),
                timeout=self.valves.TIMEOUT,
            )

            if response.status_code == 200:
                api_response = response.json()
                models = []

                # Handle OpenAI-style response
                if "data" in api_response:
                    for model_data in api_response["data"]:
                        model_id = model_data.get("id", "unknown")
                        model_name = model_data.get("id", model_id)

                        # Clean up model name for display
                        display_name = (
                            model_name.replace("/", "-").replace("_", " ").title()
                        )

                        models.append(
                            {
                                "id": f"{self.model_prefix}{model_id}",
                                "name": display_name,  # FIXED: Removed duplicate prefix
                                "description": f"Model: {model_name}",
                                "type": "chat",
                                "original_id": model_id,
                            }
                        )

                # Handle simple list response
                elif isinstance(api_response, list):
                    for model_name in api_response:
                        display_name = (
                            model_name.replace("/", "-").replace("_", " ").title()
                        )
                        models.append(
                            {
                                "id": f"{self.model_prefix}{model_name}",
                                "name": display_name,  # FIXED: Removed duplicate prefix
                                "description": f"Model: {model_name}",
                                "type": "chat",
                                "original_id": model_name,
                            }
                        )

                # Handle direct model object
                elif "models" in api_response:
                    for model_data in api_response["models"]:
                        model_id = model_data.get(
                            "id", model_data.get("name", "unknown")
                        )
                        display_name = (
                            model_id.replace("/", "-").replace("_", " ").title()
                        )

                        models.append(
                            {
                                "id": f"{self.model_prefix}{model_id}",
                                "name": display_name,  # FIXED: Removed duplicate prefix
                                "description": f"Model: {model_id}",
                                "type": "chat",
                                "original_id": model_id,
                            }
                        )

                if models:
                    self._debug(
                        f"Successfully fetched {len(models)} models from API", "MODELS"
                    )
                    return models
                else:
                    self._debug(
                        "No models found in API response, using fallback", "WARNING"
                    )

            else:
                self._debug(
                    f"Models API returned HTTP {response.status_code}", "WARNING"
                )

        except requests.exceptions.RequestException as e:
            self._debug(f"Failed to fetch models from API: {str(e)}", "ERROR")
        except Exception as e:
            self._debug(f"Unexpected error fetching models: {str(e)}", "ERROR")

        # Return fallback models if API call fails
        return self._get_fallback_models()

    def _get_fallback_models(self) -> List[dict]:
        """Return fallback models if API is unavailable."""
        self._debug("Using fallback models", "FALLBACK")
        return self.fallback_models

    def pipes(self) -> List[dict]:
        """
        Return available models/pipes from My Hugging Face Space.
        This method defines what models appear in the Open WebUI interface.
        """
        # Check if we have valid cached models
        if self._is_cache_valid():
            self._debug(
                f"Using cached models ({len(self._cached_models)} models)", "CACHE"
            )
            return self._cached_models

        # Fetch models from API
        models = self._fetch_models_from_api()

        # Cache the results
        if self.valves.CACHE_MODELS:
            self._cached_models = models
            self._cache_timestamp = time.time()
            self._debug(f"Cached {len(models)} models", "CACHE")

        return models

    def _get_original_model_id(self, openwebui_model_id: str) -> str:
        """Extract original model ID from OpenWebUI model ID."""
        if openwebui_model_id.startswith(self.model_prefix):
            return openwebui_model_id[len(self.model_prefix) :]
        return openwebui_model_id

    def pipe(self, body: dict) -> Union[str, Generator[str, None, None]]:
        """
        Main request handler for chat completions.

        Args:
            body: Request body containing model and messages

        Returns:
            Response content either as string or streaming generator
        """
        try:
            model = body.get("model", f"{self.model_prefix}default")
            messages = body.get("messages", [])
            stream = body.get("stream", False)

            # Convert OpenWebUI model ID back to original model ID
            original_model_id = self._get_original_model_id(model)

            self._debug(
                f"Processing request for model: {model} -> {original_model_id}",
                "REQUEST",
            )

            # Prepare payload for the MyHuggingFace API
            payload = {
                "model": original_model_id,
                "messages": messages,
                "stream": stream,
                "temperature": body.get("temperature", DEFAULT_TEMPERATURE),
                "max_tokens": body.get("max_tokens", DEFAULT_MAX_TOKENS),
                "top_p": body.get("top_p", DEFAULT_TOP_P),
            }

            if stream:
                return self._handle_streaming(payload)
            else:
                return self._handle_non_streaming(payload)

        except Exception as e:
            self._debug(f"Request failed: {str(e)}", "ERROR")
            return f"Error: {str(e)}"

    def _handle_streaming(self, payload: dict) -> Generator[str, None, None]:
        """Handle streaming chat completion request."""
        self._check_rate_limit()

        try:
            response = requests.post(
                f"{self.valves.HUGGINGFACE_SPACE_URL}{CHAT_ENDPOINT}",
                json=payload,
                headers=self._get_headers(),
                stream=True,
                timeout=self.valves.TIMEOUT,
            )

            if response.status_code != 200:
                yield f"Error: HTTP {response.status_code}: {response.text}"
                return

            # Process streaming response
            for line in response.iter_lines():
                if line:
                    line = line.decode("utf-8").strip()

                    if line.startswith("data: "):
                        data = line[6:]  # Remove 'data: ' prefix

                        if data.strip() == "[DONE]":
                            break

                        try:
                            json_data = json.loads(data)

                            if "choices" in json_data and len(json_data["choices"]) > 0:
                                delta = json_data["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    yield content

                        except json.JSONDecodeError:
                            if self.valves.DEBUG:
                                self._debug(f"Failed to parse JSON: {data}", "ERROR")
                            continue

        except requests.exceptions.RequestException as e:
            self._debug(f"Streaming error: {str(e)}", "ERROR")
            yield f"Error: {str(e)}"

    def _handle_non_streaming(self, payload: dict) -> str:
        """Handle non-streaming chat completion request."""
        self._check_rate_limit()

        for attempt in range(self.valves.MAX_RETRIES):
            try:
                response = requests.post(
                    f"{self.valves.HUGGINGFACE_SPACE_URL}{CHAT_ENDPOINT}",
                    json=payload,
                    headers=self._get_headers(),
                    timeout=self.valves.TIMEOUT,
                )

                if response.status_code == 200:
                    result = response.json()

                    # Handle OpenAI-style response
                    if "choices" in result and len(result["choices"]) > 0:
                        return result["choices"][0]["message"]["content"]
                    # Handle simple response
                    elif "response" in result:
                        return result["response"]
                    # Handle raw response
                    else:
                        return str(result)
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text}"
                    if attempt == self.valves.MAX_RETRIES - 1:
                        return f"Error: {error_msg}"
                    self._debug(f"Attempt {attempt + 1} failed: {error_msg}", "ERROR")

            except requests.exceptions.Timeout:
                if attempt == self.valves.MAX_RETRIES - 1:
                    return "Error: Request timed out"
                self._debug(f"Attempt {attempt + 1} timed out", "ERROR")

            except requests.exceptions.ConnectionError:
                if attempt == self.valves.MAX_RETRIES - 1:
                    return "Error: Could not connect to MyHuggingFace service"
                self._debug(f"Attempt {attempt + 1} connection failed", "ERROR")

            except Exception as e:
                if attempt == self.valves.MAX_RETRIES - 1:
                    return f"Error: {str(e)}"
                self._debug(f"Attempt {attempt + 1} failed: {str(e)}", "ERROR")

        return "Error: All retry attempts failed"

    def test_connection(self) -> dict:
        """Test connection to the My Hugging Face Space."""
        try:
            response = requests.get(
                f"{self.valves.HUGGINGFACE_SPACE_URL}{HEALTH_ENDPOINT}",
                headers=self._get_headers(),
                timeout=10,
            )

            if response.status_code == 200:
                return {"status": "success", "message": "Connection successful"}
            else:
                return {
                    "status": "error",
                    "message": f"Health check failed: HTTP {response.status_code}",
                }
        except Exception as e:
            return {"status": "error", "message": f"Connection failed: {str(e)}"}

    def refresh_models(self) -> dict:
        """Manually refresh the model cache."""
        try:
            self._cached_models = None
            self._cache_timestamp = 0
            models = self.pipes()
            return {
                "status": "success",
                "message": f"Successfully refreshed {len(models)} models",
            }
        except Exception as e:
            return {"status": "error", "message": f"Failed to refresh models: {str(e)}"}
